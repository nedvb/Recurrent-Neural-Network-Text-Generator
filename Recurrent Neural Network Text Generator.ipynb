{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8YfCOg5q6LoOoifkA8JMm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedvb/Recurrent-Neural-Network-Text-Generator/blob/main/Recurrent%20Neural%20Network%20Text%20Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import string"
      ],
      "metadata": {
        "id": "o3Du51zeyJqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nedvb/ML-Project-4.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Itizhs5k9rDX",
        "outputId": "9aca0279-3033-4c98-e58c-9ccd8cb00384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ML-Project-4'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3/3), 21.71 KiB | 1.55 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ML-Project-4/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5GShJ8wVGgJ",
        "outputId": "cf3cac64-c0f8-4c90-9e21-6d192411e8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ML-Project-4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_FILE = 'alice.txt'\n",
        "SEQ_LENGTH = 100\n",
        "HIDDEN_DIM = 700\n",
        "LAYER_NUM = 3\n",
        "BATCH_SIZE = 12\n",
        "GENERATE_LENGTH = 20\n",
        "\n",
        "data = open(DATA_FILE, 'r', encoding='latin-1').read()\n",
        "\n",
        "valid_characters = string.ascii_letters + \".,! -'\" + string.digits\n",
        "char_to_int = {ch: i for i, ch in enumerate(valid_characters)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(valid_characters)}\n",
        "\n",
        "# Clean and process the text\n",
        "data_cleaned = ''.join([ch if ch in valid_characters else ' ' for ch in data])\n",
        "data_cleaned = data_cleaned.replace('  ', ' ')\n",
        "\n",
        "# Create input-output pairs\n",
        "sequences = [data_cleaned[i:i + SEQ_LENGTH + 1] for i in range(0, len(data_cleaned) - SEQ_LENGTH, SEQ_LENGTH)]\n",
        "\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        out, h = self.lstm(x, h)\n",
        "        out = self.fc(out.view(-1, self.hidden_size))\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize hidden state and cell state with zeros\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        return (hidden, cell)\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = CharLSTM(len(valid_characters), HIDDEN_DIM, len(valid_characters), LAYER_NUM)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def generate_text(model, start_char='A', length=GENERATE_LENGTH):\n",
        "    model.eval()\n",
        "    chars = [ch for ch in start_char]\n",
        "\n",
        "    # Initialize hidden state with batch size of 1\n",
        "    h = model.init_hidden(1)\n",
        "\n",
        "    x = torch.zeros(1, 1, len(valid_characters))\n",
        "    x[0, 0, char_to_int[start_char]] = 1\n",
        "\n",
        "    for _ in range(length):\n",
        "        out, h = model(x, h)\n",
        "\n",
        "        # Sample the output distribution to get the predicted character\n",
        "        out_dist = out.view(-1).div(0.8).exp()\n",
        "        top_i = torch.multinomial(out_dist, 1)[0]\n",
        "        predicted_char = int_to_char[top_i.item()]\n",
        "\n",
        "        chars.append(predicted_char)\n",
        "\n",
        "        # Update the input for the next character prediction\n",
        "        x = torch.zeros(1, 1, len(valid_characters))\n",
        "        x[0, 0, top_i] = 1\n",
        "\n",
        "        # Ensure the hidden state maintains the correct batch size\n",
        "        h = (h[0][:, :1, :], h[1][:, :1, :])\n",
        "\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(13):\n",
        "    model.train()\n",
        "    for sequence in sequences:\n",
        "        inputs = torch.zeros(1, SEQ_LENGTH, len(valid_characters))\n",
        "        targets = torch.zeros(SEQ_LENGTH, dtype=torch.long)\n",
        "\n",
        "        for i in range(SEQ_LENGTH):\n",
        "            inputs[0, i, char_to_int[sequence[i]]] = 1\n",
        "            targets[i] = char_to_int[sequence[i + 1]]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        h = model.init_hidden(1)  # Initialize hidden state for batch size of 1\n",
        "        outputs, h = model(inputs, h)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch+1}/13], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(generate_text(model, start_char='A'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_XI32Jgui9Z",
        "outputId": "179c837f-1f0a-4400-f11e-4ea848e66deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/13], Loss: 2.0126\n",
            "Epoch [2/13], Loss: 1.7146\n",
            "Epoch [3/13], Loss: 1.5286\n",
            "Epoch [4/13], Loss: 1.3076\n",
            "Epoch [5/13], Loss: 1.0793\n",
            "Epoch [6/13], Loss: 0.9466\n",
            "Epoch [7/13], Loss: 0.8881\n",
            "Epoch [8/13], Loss: 0.6729\n",
            "Epoch [9/13], Loss: 0.5512\n",
            "Epoch [10/13], Loss: 0.5147\n",
            "Epoch [11/13], Loss: 0.3811\n",
            "Epoch [12/13], Loss: 0.2594\n",
            "Epoch [13/13], Loss: 0.2433\n",
            "Alice heard the King.\n"
          ]
        }
      ]
    }
  ]
}